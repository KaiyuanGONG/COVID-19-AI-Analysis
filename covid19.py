# -*- coding: utf-8 -*-
"""covid19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LTnYj5-wCshn2_kwY_Kg-SLhPzOUXXyx

## Analyze Current Data
"""

import pandas as pd
import numpy as np

data_org = pd.read_csv("COVID19_line_list_data.csv")

data_org.head()

column_names = data_org.columns

infos = data_org.info()

descr = data_org.describe()

"""## Data Cleaning

"""

# Delete useless columns according to the requirements of the topic
dataset = data_org.drop(['case_in_country', 'id', 'Unnamed: 3', 'If_onset_approximated', 'summary', 'source', 'link'],
                        axis=1)

# Date into uniform format
dataset['reporting date'] = pd.to_datetime(dataset['reporting date'], errors='coerce')
dataset['hosp_visit_date'] = pd.to_datetime(dataset['hosp_visit_date'], errors='coerce')
dataset['symptom_onset'] = pd.to_datetime(dataset['symptom_onset'], errors='coerce')
dataset['exposure_start'] = pd.to_datetime(dataset['exposure_start'], errors='coerce')
dataset['exposure_end'] = pd.to_datetime(dataset['exposure_end'], errors='coerce')
dataset['recovered_date'] = pd.to_datetime(dataset['recovered'], errors='coerce')

# Shift specific dates in healing and death to 1 to be consistent with others
# Convert death and recovered columns to string types
dataset['death'] = dataset['death'].astype(str)
dataset['recovered'] = dataset['recovered'].astype(str)
# Use the pandasâ€™ apply function to convert all values other than non-0 and non-1 to 1
dataset['death'] = dataset['death'].apply(lambda x: 1 if x not in ['0', '1'] else int(x))
dataset['recovered'] = dataset['recovered'].apply(lambda x: 1 if x not in ['0', '1'] else int(x))

# Convert the data type back to int
dataset['death'] = dataset['death'].astype(int)
dataset['recovered'] = dataset['recovered'].astype(int)
# The column of symptoms is divided into the presence or absence of symptoms  
dataset['symptom'] = dataset['symptom'].notna().astype(int)

# Trash value disposal
# Several dates are 1899
dataset = dataset[dataset['reporting date'].isna() | (
        (dataset['reporting date'].dt.year >= 2019) & (dataset['reporting date'].dt.year <= 2023))]
dataset = dataset[dataset['hosp_visit_date'].isna() | (
        (dataset['hosp_visit_date'].dt.year >= 2019) & (dataset['hosp_visit_date'].dt.year <= 2023))]
dataset = dataset[dataset['symptom_onset'].isna() | (
        (dataset['symptom_onset'].dt.year >= 2019) & (dataset['symptom_onset'].dt.year <= 2023))]
dataset = dataset[dataset['exposure_start'].isna() | (
        (dataset['exposure_start'].dt.year >= 2019) & (dataset['exposure_start'].dt.year <= 2023))]
dataset = dataset[dataset['exposure_end'].isna() | (
        (dataset['exposure_end'].dt.year >= 2019) & (dataset['exposure_end'].dt.year <= 2023))]
dataset = dataset[dataset['recovered_date'].isna() | (
        (dataset['recovered_date'].dt.year >= 2019) & (dataset['recovered_date'].dt.year <= 2023))]

# Missing value handling - 2 cases Delete/Padding (mean/median/plural)
# For those with few missing values, delete them directly
dataset.dropna(subset=["from Wuhan"], inplace=True)
dataset.dropna(subset=["reporting date"], inplace=True)

# For those missing values that are a bit too much, find a suitable method to fill them
# For gender, use the mode to fill in the missing values 
mode_gender = dataset['gender'].mode()[0]
dataset['gender'].fillna(mode_gender, inplace=True)
# For age, use median to supplement missing values
mode_gender = dataset['age'].mean()
dataset['age'].fillna(mode_gender, inplace=True)

# Fill with average value
dataset['reporting date'] = (dataset['reporting date'] - pd.Timestamp("2019-01-20")) // pd.Timedelta('1D')
dataset['symptom_onset'] = (dataset['symptom_onset'] - pd.Timestamp("2019-01-20")) // pd.Timedelta('1D')
dataset['hosp_visit_date'] = (dataset['hosp_visit_date'] - pd.Timestamp("2019-01-20")) // pd.Timedelta('1D')
dataset['exposure_start'] = (dataset['exposure_start'] - pd.Timestamp("2019-01-20")) // pd.Timedelta('1D')
dataset['exposure_end'] = (dataset['exposure_end'] - pd.Timestamp("2019-01-20")) // pd.Timedelta('1D')
dataset['recovered_date'] = (dataset['recovered_date'] - pd.Timestamp("2019-01-20")) // pd.Timedelta('1D')

from sklearn.impute import SimpleImputer, KNNImputer

# Fill in missing values of numeric variables using average values
num_imputer = SimpleImputer(strategy='mean')
dataset[['reporting date', 'symptom_onset', 'hosp_visit_date', 'exposure_start', 'exposure_end',
         'recovered_date']] = num_imputer.fit_transform(
    dataset[['reporting date', 'symptom_onset', 'hosp_visit_date', 'exposure_start', 'exposure_end', 'recovered_date']])

# Other follow-up treatments
dataset['from Wuhan'] = dataset['from Wuhan'].astype(int)
dataset['age'] = dataset['age'].astype(int)
# Convert male to 0,female to 1
dataset['gender'] = dataset['gender'].replace({'male': 0, 'female': 1})

"""## Data after cleaning"""

# locatioin and country are too many and not very important targets, so delete
data = dataset.drop(['location', 'country'], axis=1)

"""## Data Analysis and Visualization"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import PCA

# Calculate the correlation of all variables
correlation = data.corr()

# Create a heatmap to visualize these correlations
plt.figure(figsize=(12, 10))
sns.heatmap(correlation, annot=True, cmap='coolwarm')

# Normalize data (de-mean, unit variance)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# Performing PCA
pca = PCA()
data_pca = pca.fit_transform(data_scaled)

# Create a line graph to show the variance explained by each principal component
plt.figure(figsize=(10, 7))
plt.plot(range(1, len(data_pca[0]) + 1), np.cumsum(pca.explained_variance_ratio_))
plt.title('Explained Variance by Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')

"""For the analysis of the images, these are some points to note:

Correlation heatmap: the color of each cell indicates the correlation between two variables. The darker the color, the higher the correlation. We can find the pair of variables with the highest correlation. If these two variables are highly correlated with your target variable of interest (e.g. DEATH or RECOVERED), then they may be important features in predicting the target variable.

PCA plot: The curve represents the cumulative variance explained by the principal components. In general, we want to select enough principal components to explain most of the variance in the data. For example, if the first three principal components can explain 90% of the variance, then may only need to consider these three principal components. In a PCA plot, we should identify the point in the plot where the cumulative variance increases relatively little with the addition of new principal components. This point is what we call the "inflection point". Usually, the selection of the number of principal components is to find this "inflection point".

## Bayes Nets
"""

# Get data of all people who have visited Wuhan
wuhan_data = data[data['visiting Wuhan'] == 1]

# What is the probability that a person has COVID-19 symptoms if they have ever visited Wuhan?
symptom_rate = wuhan_data['symptom'].mean()

# What is the probability that one is indeed a patient if a person has visited Wuhan and has COVID-19 symptoms
patient_rate = wuhan_data[wuhan_data['symptom'] == 1]['from Wuhan'].mean()

# If a person ever visits Wuhan, what is the probability of his death?
death_rate = wuhan_data['death'].mean()

# If a person has visited Wuhan, what is their average recovery time?
# Here I assume that the recovery time is the difference between the reporting date and the recovered
wuhan_data['recovery_time'] = wuhan_data['recovered_date'] - wuhan_data['reporting date']
average_recovery_time = wuhan_data['recovery_time'].mean()

print("If a person has visited Wuhan, the probability of having COVID-19 symptoms is:", symptom_rate)
print("If a person has visited Wuhan and has COVID-19 symptoms, the probability of being a patient is:", patient_rate)
print("If a person has visited Wuhan, the probability of death is:", death_rate)
print("If a person has visited Wuhan, the average recovery time is:", average_recovery_time)


"""## Machine Learning"""

# Analysis outcome died / recorvered
overlap_rows = data[(data['death'] == 0) & (data['recovered'] == 1)]

print('Number of overlapping rows:', len(overlap_rows))

if len(overlap_rows) == 0:
    print("The 'death' and 'recovered' columns are mutually exclusive.")
else:
    print("The 'death' and 'recovered' columns are not mutually exclusive.")

# Create a new column 'outcome'
data['outcome'] = 'other'
data.loc[data['death'] == 1, 'outcome'] = 'death'
data.loc[data['recovered'] == 1, 'outcome'] = 'recovered'
data.info()

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

# outcome
# Splitting the training set and test set
# Partitioning the dataset into features and target variables
X = data.drop('outcome', axis=1)
y = data['outcome']

# Dividing the data set into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Using K-NN classifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
knn_predictions = knn.predict(X_test)

# Using Bayesian classifier
nb = GaussianNB()
nb.fit(X_train, y_train)
nb_predictions = nb.predict(X_test)

# Validate prediction results
# Calculate the confusion matrix ï¼Œaccuracy, precision, recall, and F1
from sklearn import metrics

# Calculating the metrics for the K-NN model
print("K-NN Metrics:")
print("Confusion Matrix:\n", metrics.confusion_matrix(y_test, knn_predictions))
print("Accuracy:", metrics.accuracy_score(y_test, knn_predictions))
print("Precision:", metrics.precision_score(y_test, knn_predictions, average='weighted'))
print("Recall:", metrics.recall_score(y_test, knn_predictions, average='weighted'))
print("F1 Score:", metrics.f1_score(y_test, knn_predictions, average='weighted'))

# Calculating metrics for Bayesian models
print("\nNaive Bayes Metrics:")
print("Confusion Matrix:\n", metrics.confusion_matrix(y_test, nb_predictions))
print("Accuracy:", metrics.accuracy_score(y_test, nb_predictions))
print("Precision:", metrics.precision_score(y_test, nb_predictions, average='weighted'))
print("Recall:", metrics.recall_score(y_test, nb_predictions, average='weighted'))
print("F1 Score:", metrics.f1_score(y_test, nb_predictions, average='weighted'))

# Visualization confusion matrix
# Calculate the confusion matrix
cm_knn = metrics.confusion_matrix(y_test, knn_predictions)
cm_nb = metrics.confusion_matrix(y_test, nb_predictions)

# Create a new matplotlib image
plt.figure(figsize=(10, 4))

# Draw the confusion matrix of K-NN on the first subgraph
plt.subplot(1, 2, 1)
sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues')
plt.title('K-NN Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')

# Draw the Bayesian confusion matrix on the second subgraph
plt.subplot(1, 2, 2)
sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues')
plt.title('Naive Bayes Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')

# Display image
plt.tight_layout()
plt.show()

"""Confusion Matrix Understanding:

- The elements on the diagonal represent the number of samples correctly classified, which are the true positive (TP) and true negative (TN) classes.


- The off-diagonal elements represent the number of samples incorrectly classified, which are the false positive (FP) and false negative (FN) classes.


- Each row of the confusion matrix corresponds to an actual class, and each column corresponds to a predicted class.

- Accuracy is the proportion of correctly predicted samples (TP and TN) out of the total samples.

- Precision is the proportion of correctly predicted positive samples (TP) out of all samples predicted as positive (TP and FP).

- Recall is the proportion of correctly predicted positive samples (TP) out of all actual positive samples (TP and FN).

- The F1 Score is the summed average of precision and recall, and can be used as a composite metric; when both precision and recall are higher, the F1 Score will also be higher.
"""

# Predicting age
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Encoding the outcome text column
# Set the 'age' column as the target variable and the other columns as features
X = pd.get_dummies(data.drop('age', axis=1))
y = data['age']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)

lr = LinearRegression()
lr.fit(X_train, y_train)
lr_predictions = lr.predict(X_test)

mse = mean_squared_error(y_test, lr_predictions)
print('Mean Squared Error:', mse)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(y_test, lr_predictions)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.xlabel('True Age')
plt.ylabel('Predicted Age')
plt.title('True Age vs Predicted Age')
plt.show()

"""All as eigenvalues does not work well, reselect eigenvalues"""

# Linear regression of the reselected eigenvalues
X = data[['death', 'symptom']]
y = data['age']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr = LinearRegression()
lr.fit(X_train, y_train)
lr_predictions = lr.predict(X_test)

mse = mean_squared_error(y_test, lr_predictions)
print('Mean Squared Error:', mse)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(y_test, lr_predictions)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.xlabel('True Age')
plt.ylabel('Predicted Age')
plt.title('True Age vs Predicted Age')
plt.show()

# Calculation and correlation of individual variables
corr_age = data.corr()['age']
print(corr_age)

# Select death and symptoms
# Using Random Forest Regression
from sklearn.ensemble import RandomForestRegressor

# sSelecting 'death' and 'symptom' as a feature
X = data[['death', 'symptom']]

# Dividing the data set into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Using Random Forest Regression Models
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_predictions = rf.predict(X_test)

# Calculate the mean square error
mse = mean_squared_error(y_test, rf_predictions)
print('Mean Squared Error:', mse)

import matplotlib.pyplot as plt
from sklearn.metrics import r2_score

# Calculate R^2 
r2 = r2_score(y_test, rf_predictions)
print(r2)

# Plot the scatter plot of actual versus predicted values
plt.scatter(y_test, rf_predictions)
plt.xlabel('Actual Age')
plt.ylabel('Predicted Age')
plt.title(f'Random Forest Regression (MSE: {mse:.2f}, R^2: {r2:.2f})')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')
plt.show()

"""The profile coefficient is a common metric for assessing the quality of clustering results. It has a value between -1 and 1. The higher the value, the better the clustering result.

In determining the best number of clusters, we can try different numbers of clusters and then calculate the contour coefficient under each number of clusters, and choose the one with the largest contour coefficient as the best number of clusters.


"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Store the silhouette score for each number of clusters
silhouette_scores = []

# Try a different number of clusters
for n_clusters in range(2, 11):
    # Create a KMeans instance
    kmeans = KMeans(n_clusters=n_clusters, random_state=0)

    # Training Model
    kmeans.fit(data_scaled)

    # Calculation of silhouette score
    score = silhouette_score(data_scaled, kmeans.labels_)

    # Storage silhouette score
    silhouette_scores.append(score)

# Draw the silhouette scores under each cluster number
plt.figure(figsize=(10, 7))
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score vs Number of Clusters')
plt.grid()

from sklearn.decomposition import PCA

# Create a KMeans instance and set the number of clusters to 8
kmeans = KMeans(n_clusters=8, random_state=0)

# Training Models
kmeans.fit(data_scaled)

# Dimensionality reduction using PCA
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)

# Use different colors to indicate individual clusters
plt.figure(figsize=(10, 7))
for i in range(8):
    plt.scatter(data_pca[kmeans.labels_ == i, 0], data_pca[kmeans.labels_ == i, 1], label='Cluster ' + str(i + 1))
plt.legend()
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Clustering Results with 8 clusters')

# 3D visualization
from mpl_toolkits.mplot3d import Axes3D

# Mapping the data to the first three principal components
pca_3 = PCA(n_components=3)
data_pca_3 = pca_3.fit_transform(data_scaled)

# Create 3D drawings
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Extracting each principal component
x = data_pca_3[:, 0]
y = data_pca_3[:, 1]
z = data_pca_3[:, 2]

# Assign colors to each point based on the cluster's label
ax.scatter(x, y, z, c=kmeans.labels_, cmap='viridis')

# Set axis labels
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')

# Display image
plt.show()

"""##  Improving the results and Theoretical formalism

A. The data is unbalanced. You can balance it by reducing randomly the majority class. 
Assume that you extract randomly samples that are balanced. How the prediction results 
will change?

It is possible that some important information is discarded. Therefore, the prediction results may vary, depending on whether the discarded data is critical or not

B. How you can better mange the missing values?

As I have used in the previous exercises, it is common to remove rows with missing values, fill using statistical values (e.g. mean, plurality, median), and fill using models for prediction. You can also use K-NN for padding
"""

# Linear regression was re-run after supplementation with K-NN
# Select relevant features
features = data[['age', 'symptom', 'death']]

# Using KNNImputer
imputer = KNNImputer(n_neighbors=5)

# Fill data with fit_transform
data_filled = imputer.fit_transform(features)

# Go back to DataFrame
data_filled = pd.DataFrame(data_filled, columns=features.columns)

# Update the populated data columns back to the original DataFrame
data.update(data_filled)

# Select linear regression of eigenvalues
X = data[['death', 'symptom', 'visiting Wuhan', 'recovered']]
y = data['age']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr = LinearRegression()
lr.fit(X_train, y_train)
lr_predictions = lr.predict(X_test)

mse = mean_squared_error(y_test, lr_predictions)
print('Mean Squared Error:', mse)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(y_test, lr_predictions)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)
plt.xlabel('True Age')
plt.ylabel('Predicted Age')
plt.title('True Age vs Predicted Age')
plt.show()

"""C. To find the best parameters for the models, the Grid-search algorithm can be used 
which is available in scikit-learn library. Explain the algorithm and use it for the learning models to find the best parameters.

Algorithm principle: Grid Search is a method used to determine the optimal hyperparameters. It generates a grid of hyperparameters, then tries all combinations and finally returns the best performing set of hyperparameters:

-Setting a range of possible values for the hyperparameters;

-Performing model training and validation for each set of hyperparameter combinations;

-Selecting the set of hyperparameters with the best validation results.
"""

# Grid search improvement K-NN
from sklearn.model_selection import GridSearchCV

# For the K-NN model, the parameters we can adjust may be n_neighbors
knn_params = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

# Find the optimal parameters using GridSearchCV
knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)
knn_grid.fit(X_train, y_train)

# Output optimal parameters
print("Best Parameters for K-NN: ", knn_grid.best_params_)

"""D. Give the algorithmically (mathematical) formalism of the method which give the best results. Explain all the parameters of the used method and their impact on the results. Some comparison with public results should me made to conclude the project.

"""
